import asyncio
import random
import time
from playwright.async_api import async_playwright
import gspread
from datetime import datetime

# --- âš™ï¸ ì„¤ì • ---
SERVICE_ACCOUNT_FILE = 'service_account2020.json' 
SHEET_URL = 'https://docs.google.com/spreadsheets/d/1omDVgsy4qwCKZMbuDLoKvJjNsOU1uqkfBqZIM7euezk/edit?gid=0#gid=0'

TARGET_GALLERIES = [
    {"name": "í•™ì ì€í–‰ì œê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=acbs", "mo": "https://m.dcinside.com/board/acbs"},
    {"name": "í† ìµê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=toeic", "mo": "https://m.dcinside.com/board/toeic"},
    {"name": "4ë…„ì œëŒ€í•™ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=4year_university", "mo": "https://m.dcinside.com/board/4year_university"},
    {"name": "í¸ì…ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=admission", "mo": "https://m.dcinside.com/board/admission"}
]

def get_korean_position(env, page_type, raw_pos, img_src):
    raw = str(raw_pos).lower()
    if not img_src: return "í…ìŠ¤íŠ¸ë°°ë„ˆ"
    if "icon" in raw or "float" in raw or "pop-layer" in raw: return "ì•„ì´ì½˜ë°°ë„ˆ"
    
    if env == "PC":
        if page_type == "ë³¸ë¬¸": 
            if "bottom" in raw or "btm" in raw: return "í•˜ë‹¨ë°°ë„ˆ"
            return "ê²Œì‹œê¸€ë°°ë„ˆ"
        else: 
            if "right" in raw or "wing" in raw: return "ìš°ì¸¡ë°°ë„ˆ"
            if "left" in raw: return "ì¢Œì¸¡ë°°ë„ˆ"
            if "bottom" in raw or "btm" in raw: return "í•˜ë‹¨ë°°ë„ˆ"
            return "ìƒë‹¨ë°°ë„ˆ"
    else: 
        if page_type == "ë³¸ë¬¸":
            if "bottom" in raw or "btm" in raw: return "í•˜ë‹¨ë°°ë„ˆ"
            return "ê²Œì‹œê¸€ë°°ë„ˆ"
        else:
            if "bottom" in raw or "btm" in raw: return "í•˜ë‹¨ë°°ë„ˆ"
            return "ìƒë‹¨ë°°ë„ˆ"

# ğŸ”— ìƒˆ ì°½ì„ ë„ì›Œ ì£¼ì†Œë¥¼ ë¶™ì—¬ë„£ê³  ëœë”© URL ë‚šì•„ì±„ê¸°
async def get_final_landing_url(context, redirect_url):
    if not redirect_url or not redirect_url.startswith("http"): return redirect_url
    if "addc.dcinside" not in redirect_url and "NetInsight" not in redirect_url: return redirect_url
    try:
        temp_page = await context.new_page()
        # ì§ì ‘ í´ë¦­í•˜ì§€ ì•Šê³  ìƒˆ ì°½ì—ì„œ URLë¡œ ë°”ë¡œ ì´ë™ (ë¹ ë¥¸ íƒˆì·¨ë¥¼ ìœ„í•´ commit ì‚¬ìš©)
        await temp_page.goto(redirect_url, wait_until="commit", timeout=4000)
        final_url = temp_page.url
        await temp_page.close()
        return final_url
    except:
        return redirect_url

# âš¡ ì†ë„ í­ë°œì˜ í•µì‹¬: ê´‘ê³ ì™€ ìƒê´€ì—†ëŠ” ì°Œêº¼ê¸° íŒŒì¼ ì ˆëŒ€ ë‹¤ìš´ë¡œë“œ ê¸ˆì§€
async def block_unnecessary_resources(route):
    # ê´‘ê³  ë„ë©”ì¸ì´ ì•„ë‹Œ ì¼ë°˜ ì´ë¯¸ì§€, í°íŠ¸, ë¯¸ë””ì–´ ì°¨ë‹¨
    req_url = route.request.url
    if route.request.resource_type in ["font", "media", "stylesheet"]:
        await route.abort()
    elif route.request.resource_type == "image" and not any(k in req_url for k in ["dcinside", "toast.com", "ads"]):
        await route.abort()
    else:
        await route.continue_()

async def capture_all_visible_ads(context, page, env, gallery_name, page_type):
    collected = []
    seen_keys = set()
    today_str = datetime.now().strftime("%Y-%m-%d")
    prefix = f"[{env}|{gallery_name[:4]}|{page_type}]"
    
    valid_refreshes = 0
    attempt = 0
    
    while valid_refreshes < 10 and attempt < 30:
        attempt += 1
        found_ad_in_this_round = False
        current_round = valid_refreshes + 1
        ad_count_in_round = 0 # ì´ë²ˆ ìƒˆë¡œê³ ì¹¨ì—ì„œ ì°¾ì€ ê´‘ê³  ê°œìˆ˜
        
        try:
            await page.reload(wait_until="domcontentloaded", timeout=12000)
            await asyncio.sleep(2) 
            if page_type == "ë³¸ë¬¸":
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
                await asyncio.sleep(1)
        except: pass

        for frame in page.frames:
            try:
                ads = await frame.locator("a").all()
                for ad in ads:
                    href = await ad.get_attribute("href") or ""
                    img = ad.locator("img")
                    img_src = await img.first.evaluate("node => node.src") if await img.count() > 0 else ""
                    raw_pos = await ad.evaluate("node => { let p = node.closest('div'); return p ? p.className : 'unknown'; }")
                    
                    if "google" in href.lower() or "adsrvr.org" in href.lower() or "criteo" in href.lower(): continue
                    if "googleactiveview" in str(raw_pos).lower(): continue
                    if "dc/w/images" in img_src or "info_polic" in href or "close" in img_src.lower(): continue
                    if href == "#" or "javascript" in href.lower() or not href: continue
                        
                    is_ad = any(k in href or k in (img_src or "") for k in ["addc.dcinside", "NetInsight", "nstatic.dcinside", "toast.com"])
                    
                    if is_ad:
                        found_ad_in_this_round = True
                        key = img_src if img_src else href
                        
                        if key not in seen_keys:
                            seen_keys.add(key)
                            ad_count_in_round += 1
                            final_url = await get_final_landing_url(context, href)
                            text_val = (await img.first.get_attribute("alt") if img_src else await ad.inner_text()) or "ì´ë¯¸ì§€ ë°°ë„ˆ"
                            korean_pos = get_korean_position(env, page_type, raw_pos, img_src)
                            
                            # ë¡œê·¸ ì§ê´€ì„± ê°œì„ : [ëª‡ ë²ˆì§¸ ìƒˆë¡œê³ ì¹¨] - [ëª‡ ë²ˆì§¸ ë°°ë„ˆ]
                            print(f"âœ… {prefix} [{current_round}íšŒì°¨ ìƒˆë¡œê³ ì¹¨ - {ad_count_in_round}ë²ˆì§¸ ë°œê²¬] {korean_pos}")
                            
                            collected.append({
                                "date": today_str, "gallery": gallery_name, "env": env,
                                "pos": korean_pos, "url": final_url, "img": img_src, "text": text_val.strip()
                            })
            except: continue
        
        if found_ad_in_this_round:
            valid_refreshes += 1
            
    return collected

async def run_scraper_task(sem, context, env, target):
    async with sem:
        # ë´‡ ì°¨ë‹¨ ë°©ì§€ìš© ì•½ê°„ì˜ ì¶œë°œ ë”œë ˆì´
        await asyncio.sleep(random.uniform(0, 1.5)) 
        page = await context.new_page()
        await page.route("**/*", block_unnecessary_resources)
        data = []
        try:
            url = target['pc'] if env == "PC" else target['mo']
            await page.goto(url, wait_until="domcontentloaded", timeout=15000)
            data.extend(await capture_all_visible_ads(context, page, env, target['name'], "ë¦¬ìŠ¤íŠ¸"))
            
            post = page.locator("tr.us-post:not(.notice) td.gall_tit > a:not(.reply_numbox)").first if env == "PC" else page.locator("ul.gall-detail-lst li:not(.notice) .gall-detail-lnktit a").first
            if await post.count() > 0:
                await post.click()
                await asyncio.sleep(1.5)
                data.extend(await capture_all_visible_ads(context, page, env, target['name'], "ë³¸ë¬¸"))
        except: pass
        finally: await page.close()
        return data

async def main():
    print("==================================================")
    print("ğŸš€ ë””ì‹œì¸ì‚¬ì´ë“œ ê´‘ê³  ì´ˆê³ ì† ë³‘ë ¬ ìˆ˜ì§‘ (êµ¬ê¸€ API ë¶„í•  ì—…ë¡œë“œ ì ìš©)")
    print("==================================================")
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=["--disable-blink-features=AutomationControlled", "--disable-dev-shm-usage", "--no-sandbox"]
        )
        ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
        
        pc_context = await browser.new_context(viewport={"width": 1920, "height": 1080}, user_agent=ua)
        mo_context = await browser.new_context(**p.devices['iPhone 13']) 

        # ğŸ”¥ 8ê°œ í™˜ê²½(PC 4ê°œ, MO 4ê°œ)ì„ í•œ ë²ˆì— ëª¨ë‘ ë™ì‹œì— ì¶œë°œì‹œí‚µë‹ˆë‹¤. (ì†ë„ ê·¹ëŒ€í™”)
        sem = asyncio.Semaphore(8)

        tasks = []
        for target in TARGET_GALLERIES:
            tasks.append(run_scraper_task(sem, pc_context, "PC", target))
            tasks.append(run_scraper_task(sem, mo_context, "MO", target))

        results = await asyncio.gather(*tasks, return_exceptions=True)
        await browser.close()
        
        all_final_data = [item for sublist in results if isinstance(sublist, list) for item in sublist]

    # ğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ 30ê°œ ë¶„í•  ì—…ë¡œë“œ (API ì¿¼í„° ì—ëŸ¬ ì™„ë²½ ì°¨ë‹¨)
    if all_final_data:
        print(f"\nğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ ì •ë¦¬ ë° ë¶„í•  ì—…ë¡œë“œë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤... (ì´ {len(all_final_data)}ê±´)")
        gc = gspread.service_account(filename=SERVICE_ACCOUNT_FILE)
        ws = gc.open_by_url(SHEET_URL).get_worksheet(0)
        
        all_rows = ws.get_all_values()
        today_str = datetime.now().strftime("%Y-%m-%d")
        new_sheet_data = [["ë‚ ì§œ", "ê°¤ëŸ¬ë¦¬ëª…", "í™˜ê²½", "ìœ„ì¹˜", "URL", "ì´ë¯¸ì§€", "í…ìŠ¤íŠ¸ë¬¸êµ¬"]]
        
        if all_rows:
            for row in all_rows[1:]:
                if len(row) > 0 and row[0] != today_str: 
                    new_sheet_data.append(row)
                    
        for d in all_final_data:
            new_sheet_data.append([d['date'], d['gallery'], d['env'], d['pos'], d['url'], d['img'], d['text']])
            
        # ê¸°ì¡´ ë‚´ìš© í•œ ë²ˆì— ì‹¹ ì§€ìš°ê¸°
        ws.clear()
        
        # ğŸ”¥ ë°ì´í„°ë¥¼ 30ê°œì”© ìª¼ê°œì„œ ì—…ë¡œë“œ
        chunk_size = 30
        total_chunks = (len(new_sheet_data) // chunk_size) + 1
        
        print(f"ğŸ“¦ ë°ì´í„°ë¥¼ {total_chunks}ê°œì˜ ë©ì–´ë¦¬ë¡œ ë‚˜ëˆ„ì–´ ì•ˆì „í•˜ê²Œ ì—…ë¡œë“œí•©ë‹ˆë‹¤.")
        for i in range(0, len(new_sheet_data), chunk_size):
            chunk = new_sheet_data[i : i + chunk_size]
            ws.append_rows(chunk)
            print(f"   â–¶ï¸ {i + len(chunk)} / {len(new_sheet_data)} ê±´ ì—…ë¡œë“œ ì™„ë£Œ...")
            time.sleep(1.5) # êµ¬ê¸€ API ì“°ê¸° ì œí•œ(Rate Limit) ë°©ì§€ìš© ê¿€ë§› íœ´ì‹
            
        print("\nğŸ‰ ëª¨ë“  ë¶„í•  ì—…ë¡œë“œ ë° ì´ˆê³ ì† ìˆ˜ì§‘ì´ ì™„ë²½í•˜ê²Œ ëë‚¬ìŠµë‹ˆë‹¤!")
    else:
        print("\nâŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())
