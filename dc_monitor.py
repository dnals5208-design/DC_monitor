import asyncio
import random
import time
import os
import re
from playwright.async_api import async_playwright
import gspread
from datetime import datetime, timedelta, timezone

# --- âš™ï¸ ì„¤ì • ---
SERVICE_ACCOUNT_FILE = 'service_account2020.json'
SHEET_URL = 'https://docs.google.com/spreadsheets/d/1omDVgsy4qwCKZMbuDLoKvJjNsOU1uqkfBqZIM7euezk/edit?gid=0#gid=0'

ALL_GALLERIES = [
    {"name": "ìê²©ì¦ê°¤ëŸ¬ë¦¬","pc":"https://gall.dcinside.com/board/lists/?id=coq","mo":"https://m.dcinside.com/board/coq"},
    {"name": "í¸ì…ê°¤ëŸ¬ë¦¬","pc":"https://gall.dcinside.com/board/lists/?id=admission","mo":"https://m.dcinside.com/board/admission"},
    {"name": "ì •ë³‘ê¶Œê°¤ëŸ¬ë¦¬","pc":"https://gall.dcinside.com/mgallery/board/lists/?id=jeongbyeongkwon","mo":"https://m.dcinside.com/board/jeongbyeongkwon"},
    {"name": "í•™ì ì€í–‰ì œê°¤ëŸ¬ë¦¬","pc":"https://gall.dcinside.com/board/lists/?id=acbs","mo":"https://m.dcinside.com/board/acbs"},
    {"name": "4ë…„ì œëŒ€í•™ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=4year_university", "mo": "https://m.dcinside.com/board/4year_university"},
    {"name": "ë²•í•™ì „ë¬¸ëŒ€í•™ì›ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=lawschool", "mo": "https://m.dcinside.com/board/lawschool"},
    {"name": "ê³µë¬´ì›ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=government_new1", "mo": "https://m.dcinside.com/board/government_new1"},
    {"name": "êµ°ë¬´ì›ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/mgallery/board/lists/?id=soider", "mo": "https://m.dcinside.com/board/soider"},
    {"name": "ê³µì¸ì¤‘ê°œì‚¬ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/mgallery/board/lists/?id=bokdukbang", "mo": "https://m.dcinside.com/board/bokdukbang"},
    {"name": "ì£¼íƒê´€ë¦¬ì‚¬ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/mgallery/board/lists/?id=housingmanager", "mo": "https://m.dcinside.com/board/housingmanager"},
    {"name": "ê°ì •í‰ê°€ì‚¬ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/mgallery/board/lists/?id=gampyeong", "mo": "https://m.dcinside.com/board/gampyeong"},
    {"name": "í–‰ì •ì‚¬ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/mgallery/board/lists/?id=hangjungsa", "mo": "https://m.dcinside.com/board/hangjungsa"},
    {"name": "ì†Œë°©ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/mgallery/board/lists/?id=firefighter", "mo": "https://m.dcinside.com/board/firefighter"},
    {"name": "ìˆœê²½ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=policeofficer", "mo": "https://m.dcinside.com/board/policeofficer"},
    {"name": "ì„ìš©ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=tce", "mo": "https://m.dcinside.com/board/tce"},
    {"name": "í† ìµê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=toeic", "mo": "https://m.dcinside.com/board/toeic"},
    {"name": "ì¼ì–´ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists?id=japanese", "mo": "https://m.dcinside.com/board/japanese"},
    {"name": "ì˜ì–´ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists?id=English", "mo": "https://m.dcinside.com/board/English"},
    {"name": "ì˜ì–´íšŒí™”ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists?id=ec", "mo": "https://m.dcinside.com/board/ec"},
    {"name": "ì¤‘êµ­ì–´ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=chinese", "mo": "https://m.dcinside.com/board/chinese"},
    {"name": "ì„¸ë¬´ì‚¬ê°¤ëŸ¬ë¦¬","pc":"https://gall.dcinside.com/board/lists/?id=cta","mo":"https://m.dcinside.com/board/cta"},
    {"name": "íšŒê³„ì‚¬ê°¤ëŸ¬ë¦¬","pc":"https://gall.dcinside.com/board/lists/?id=cpa","mo":"https://m.dcinside.com/board/cpa"}
]

CHUNK_INDEX = int(os.getenv("CHUNK_INDEX", 0))
TOTAL_CHUNKS = int(os.getenv("TOTAL_CHUNKS", 1))

base_size = len(ALL_GALLERIES) // TOTAL_CHUNKS
remainder = len(ALL_GALLERIES) % TOTAL_CHUNKS

if CHUNK_INDEX < remainder:
    start_idx = CHUNK_INDEX * (base_size + 1)
    end_idx = start_idx + (base_size + 1)
else:
    start_idx = remainder * (base_size + 1) + (CHUNK_INDEX - remainder) * base_size
    end_idx = start_idx + base_size

TARGET_GALLERIES = ALL_GALLERIES[start_idx:end_idx]

# --- ğŸ“Š êµ¬ê¸€ ì‹œíŠ¸ ì•ˆì „ ë°°ì¹˜ ì—…ë¡œë“œ ---
def safe_batch_upload(ws, data_chunk):
    if not data_chunk: return
    rows = [[d['date'], d['gallery'], d['env'], d['pos'], d['url'], d['img'], d['text']] for d in data_chunk]
    for i in range(0, len(rows), 30):
        try:
            ws.append_rows(rows[i:i+30])
            time.sleep(1.5)
        except Exception: time.sleep(5)

# --- ğŸ“¤ ë¹„ë™ê¸° ì—…ë¡œë” ì›Œì»¤ ---
async def uploader_worker(queue, ws):
    buffer = []
    while True:
        item = await queue.get()
        if item is None: break
        buffer.append(item)
        if len(buffer) >= 100:
            print(f"ğŸš€ [ì„œë²„ {CHUNK_INDEX+1}] 100ê°œ ë„ë‹¬! ì¤‘ê°„ ì—…ë¡œë“œ ì¤‘...")
            await asyncio.to_thread(safe_batch_upload, ws, buffer)
            buffer.clear()
        queue.task_done()
    if buffer:
        await asyncio.to_thread(safe_batch_upload, ws, buffer)

# --- ğŸ“ ìœ„ì¹˜ëª… ì •ë°€ íŒë… (URL íŒŒì‹± + fallback) ---
def get_korean_position(env, page_type, raw_pos, is_image, raw_href, urls_text):
    target_url = raw_href.split('?')[0].lower()
    pos_result = ""

    # 1ì°¨: ë””ì‹œ ê´‘ê³  URL êµ¬ì¡°ì—ì„œ ìœ„ì¹˜ íŒŒì‹± (ê°€ì¥ ì •í™•)
    if "click/dcinside" in target_url:
        try:
            parts = target_url.split('/')
            last_part = parts[-1]
            if '@' in last_part:
                page_str, pos_gallery = last_part.split('@', 1)
                pos_str = pos_gallery.split('_')[0]
                page_kr = "ë¦¬ìŠ¤íŠ¸" if page_str == "list" else "ë³¸ë¬¸"
                if pos_str == "top": pos_kr = "ìƒë‹¨ë°°ë„ˆ"
                elif pos_str == "middle": pos_kr = "ì¤‘ë‹¨ë°°ë„ˆ"
                elif pos_str in ["bottom", "reply"]: pos_kr = "í•˜ë‹¨ë°°ë„ˆ"
                elif pos_str == "left": pos_kr = "ì¢Œì¸¡ë°°ë„ˆ"
                elif pos_str == "right": pos_kr = "ìš°ì¸¡ë°°ë„ˆ"
                elif "auto" in pos_str: pos_kr = "ì§¤ë°©ë°°ë„ˆ"
                elif "icon" in pos_str or "float" in pos_str: pos_kr = "ì•„ì´ì½˜ë°°ë„ˆ"
                else: pos_kr = "ë°°ë„ˆ"
                pos_result = f"{page_kr} {pos_kr}"
        except: pass

    # 2ì°¨: DOM í´ë˜ìŠ¤ëª… + href í…ìŠ¤íŠ¸ ê¸°ë°˜ fallback
    if not pos_result:
        raw = (str(raw_pos) + " " + str(urls_text)).lower()
        if not is_image: return "í…ìŠ¤íŠ¸ë°°ë„ˆ"
        if "icon" in raw or "float" in raw or "pop-layer" in raw: return "ì•„ì´ì½˜ë°°ë„ˆ"

        page_kr = "ë¦¬ìŠ¤íŠ¸" if page_type == "ë¦¬ìŠ¤íŠ¸" else "ë³¸ë¬¸"
        if env == "PC":
            if page_type == "ë³¸ë¬¸":
                pos_result = f"{page_kr} í•˜ë‹¨ë°°ë„ˆ" if "bottom" in raw or "btm" in raw else f"{page_kr} ê²Œì‹œê¸€ë°°ë„ˆ"
            else:
                if "right" in raw or "wing" in raw: pos_result = f"{page_kr} ìš°ì¸¡ë°°ë„ˆ"
                elif "left" in raw: pos_result = f"{page_kr} ì¢Œì¸¡ë°°ë„ˆ"
                else: pos_result = f"{page_kr} í•˜ë‹¨ë°°ë„ˆ" if "bottom" in raw or "btm" in raw else f"{page_kr} ìƒë‹¨ë°°ë„ˆ"
        else:
            if page_type == "ë³¸ë¬¸": pos_result = f"{page_kr} í•˜ë‹¨ë°°ë„ˆ" if "bottom" in raw or "btm" in raw else f"{page_kr} ê²Œì‹œê¸€ë°°ë„ˆ"
            else: pos_result = f"{page_kr} í•˜ë‹¨ë°°ë„ˆ" if "bottom" in raw or "btm" in raw else f"{page_kr} ìƒë‹¨ë°°ë„ˆ"

    # ğŸ”¥ ì˜ì—­ëª… ê°•ì œ ì •ìƒí™”: ë¦¬ìŠ¤íŠ¸ì—ì„œ ìˆ˜ì§‘í–ˆëŠ”ë° 'ë³¸ë¬¸'ì´ ì„ì—¬ ë‚˜ì˜¤ë©´ 'ë¦¬ìŠ¤íŠ¸ ê³µì§€'ë¡œ êµì •
    if page_type == "ë¦¬ìŠ¤íŠ¸" and "ë³¸ë¬¸" in pos_result:
        return "ë¦¬ìŠ¤íŠ¸ ê³µì§€"

    return pos_result

# --- ğŸ”— ìµœì¢… ëœë”© URL ì¶”ì  ---
async def get_final_landing_url(context, redirect_url, referer_url):
    if not redirect_url or not redirect_url.startswith("http"): return redirect_url
    if "addc.dc" not in redirect_url and "netinsight" not in redirect_url: return redirect_url

    try:
        temp = await context.new_page()
        await temp.route("**/*", lambda route: route.abort() if route.request.resource_type in ["image", "media", "font", "stylesheet"] else route.continue_())
        try:
            await temp.goto(redirect_url, referer=referer_url, wait_until="commit", timeout=5000)
        except: pass

        for _ in range(25):
            current_url = temp.url
            if "addc.dc" not in current_url and "netinsight" not in current_url and current_url != "about:blank":
                break
            await asyncio.sleep(0.2)

        final_url = temp.url
        await temp.close()
        return final_url
    except: return redirect_url

# --- ğŸš« ë¶ˆí•„ìš” ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨ ---
async def block_resources(route):
    if route.request.resource_type in ["font", "media"]: await route.abort()
    else: await route.continue_()

# --- ğŸ” ê´‘ê³  íƒìƒ‰ í•µì‹¬ ì—”ì§„ ---
# MO ë³¸ë¬¸ì§¤ë°©ì€ í˜ì´ì§€ ìµœìƒë‹¨(ì œëª© ë°”ë¡œ ì•„ë˜)ì— ìœ„ì¹˜ â†’ ìŠ¤í¬ë¡¤ ì „ ìµœìƒë‹¨ ëŒ€ê¸°ê°€ í•µì‹¬
# PC: 40íšŒ ìœ íš¨ / 80íšŒ ìµœëŒ€, MO: 50íšŒ ìœ íš¨ / 100íšŒ ìµœëŒ€
async def capture_ads(context, page, env, gallery, page_type):
    collected = []
    seen = set()

    KST = timezone(timedelta(hours=9))
    today = datetime.now(KST).strftime("%Y-%m-%d")
    prefix = f"[{env}|{gallery[:4]}|{page_type}]"

    max_valid = 50 if env == "MO" else 40
    max_total = max_valid * 2

    valid_attempts = 0
    total_attempts = 0

    while valid_attempts < max_valid and total_attempts < max_total:

        # ğŸ”¥ 10íšŒë§ˆë‹¤ ì¿ í‚¤ + localStorage/sessionStorage ì™„ì „ ì´ˆê¸°í™” (í”„ë¦¬í€€ì‹œ ìº¡ ìš°íšŒ)
        if total_attempts > 0 and total_attempts % 10 == 0:
            try:
                await context.clear_cookies()
                await page.evaluate("""() => {
                    try { window.localStorage.clear(); } catch(e) {}
                    try { window.sessionStorage.clear(); } catch(e) {}
                }""")
                print(f"ğŸ”„ {prefix} [ì„¸ì…˜ ê°±ì‹ ] ì¿ í‚¤+ìŠ¤í† ë¦¬ì§€ ì´ˆê¸°í™” ì™„ë£Œ ({total_attempts}íšŒì°¨)")
            except: pass

        total_attempts += 1
        found_dc_ad_in_this_round = False

        try:
            # ğŸ“Œ ìŠ¤í¬ë¡¤ ë¦¬ì…‹ â†’ ë¦¬ë¡œë“œ â†’ ìµœìƒë‹¨ ëŒ€ê¸° (ì§¤ë°© ë°°ë„ˆ ë¡œë”© ë³´ì¥)
            await page.evaluate("window.scrollTo(0, 0);")
            await page.reload(wait_until="load", timeout=12000)

            if page_type == "ë³¸ë¬¸":
                # ğŸ”¥ í•µì‹¬: ìµœìƒë‹¨(0%)ì—ì„œ 2ì´ˆ ëŒ€ê¸° â†’ ì§¤ë°© ë°°ë„ˆ iframe ë¡œë”© ì™„ë£Œ ë³´ì¥
                await asyncio.sleep(2.0)
                # ì¤‘ë‹¨~í•˜ë‹¨ ê´‘ê³ ë¥¼ ìœ„í•´ 50% â†’ 100% 2ë‹¨ê³„ë§Œ ë¹ ë¥´ê²Œ
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight * 0.5);")
                await asyncio.sleep(0.5)
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
                await asyncio.sleep(0.5)
                # ë‹¤ì‹œ ìƒë‹¨ìœ¼ë¡œ ë³µê·€ (ì§¤ë°© ë°°ë„ˆê°€ ë·°í¬íŠ¸ì— ìˆì–´ì•¼ DOM ì ‘ê·¼ ì•ˆì •)
                await page.evaluate("window.scrollTo(0, 0);")
                await asyncio.sleep(0.3)
            else:
                # ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€: 3ë‹¨ê³„ ì¾Œì† ìŠ¤í¬ë¡¤
                await asyncio.sleep(1.5)
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight / 3);")
                await asyncio.sleep(0.5)
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight / 1.5);")
                await asyncio.sleep(0.5)
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
                await asyncio.sleep(1.0)
        except: pass

        base_page_url = page.url.split('#')[0].split('?')[0].lower()

        for frame in page.frames:
            try:
                for ad in await frame.locator("a").all():

                    raw_href_attr = await ad.get_attribute("href") or ""
                    clean_href_attr = raw_href_attr.strip().lower()

                    if clean_href_attr == "#" or clean_href_attr.endswith("#"): continue
                    if "javascript:" in clean_href_attr and "window.open" not in clean_href_attr: continue
                    if "/board/lists" in clean_href_attr or "/mini/board/lists" in clean_href_attr: continue
                    if "dcad" in clean_href_attr: continue

                    # JavaScript ë™ì  href ì¶”ì¶œ (onclick ë“±)
                    raw_href = await ad.evaluate("""n => {
                        if (n.href && !n.href.includes('__CLICK__') && !n.href.includes('__click__') && !n.href.includes('null')) return n.href;
                        let oc = n.getAttribute('onclick');
                        if (oc) {
                            let m = oc.match(/['"](http[^'"]+)['"]/);
                            if (m) return m[1];
                        }
                        return n.href || '';
                    }""")
                    clean_href = raw_href.strip().lower()

                    if clean_href.endswith("#") or "/board/lists" in clean_href or "dcad" in clean_href: continue

                    # ì´ë¯¸ì§€ ì¶”ì¶œ (data-src, data-original, background-image í¬í•¨)
                    img_src = await ad.evaluate("""n => {
                        let getValidSrc = (el) => {
                            let w = el.getAttribute('width');
                            let h = el.getAttribute('height');
                            if (w && parseInt(w) <= 10) return null;
                            if (h && parseInt(h) <= 10) return null;

                            let src = el.getAttribute('data-src') || el.getAttribute('data-original') || el.src;
                            if (src && !src.includes('data:image')) return src;
                            return null;
                        };
                        let img = n.querySelector('img');
                        if (img) {
                            let valid = getValidSrc(img);
                            if (valid) return valid;
                        }
                        let bg = window.getComputedStyle(n).backgroundImage;
                        if (bg && bg !== 'none' && bg.includes('url')) {
                            let url = bg.replace(/^url\\(['"]?/, '').replace(/['"]?\\)$/, '');
                            if (!url.includes('data:image')) return url;
                        }
                        let child = n.querySelector('div, span');
                        if (child) {
                            let cbg = window.getComputedStyle(child).backgroundImage;
                            if (cbg && cbg !== 'none' && cbg.includes('url')) {
                                let url = cbg.replace(/^url\\(['"]?/, '').replace(/['"]?\\)$/, '');
                                if (!url.includes('data:image')) return url;
                            }
                        }
                        return '';
                    }""")

                    raw_pos = await ad.evaluate("n => { let p = n.closest('div'); return p ? p.className : ''; }")
                    txt = await ad.inner_text() or ""

                    clean_img = img_src.strip()
                    clean_txt = re.sub(r'<[^>]+>', '', txt).strip()

                    bad_img_urls = ["board/lists", "gall.dcinside.com", "m.dcinside.com"]
                    if clean_img and any(bad in clean_img.lower() for bad in bad_img_urls):
                        clean_img = ""

                    if clean_txt:
                        clean_txt = re.sub(r'^(AD|ad)\s*', '', clean_txt).strip()
                    if clean_txt in ["ê´‘ê³ ì•ˆë‚´", "ê°¤ëŸ¬ë¦¬", "ì´ë¯¸ì§€ ë°°ë„ˆ", "null", "dcinside.com"]:
                        clean_txt = ""
                    if not clean_img and not clean_txt: continue

                    # ì™¸ë¶€ ë„¤íŠ¸ì›Œí¬ ê´‘ê³  í•„í„°ë§
                    external_ad_networks = ["google", "adsrvr", "criteo", "taboola", "doubleclick", "adnxs", "smartadserver", "naver.com", "ader.naver.com", "nclick", "kakao", "daum", "mobon", "exelbid"]
                    if any(k in clean_href for k in external_ad_networks): continue

                    # ì“°ë ˆê¸° ì´ë¯¸ì§€ í•„í„°ë§
                    junk_images = ["close", "x_btn", "traffic_", "default_banner", "noimage", "icon", "btn_ad_close"]
                    if clean_img and any(j in clean_img.lower() for j in junk_images): continue

                    # ë””ì‹œ ì§íŒ ê´‘ê³  íŒë³„
                    is_real_ad = False
                    if any(x in clean_href for x in ["addc.dc", "netinsight", "toast", "utm_source"]):
                        is_real_ad = True

                    if not is_real_ad:
                        continue

                    found_dc_ad_in_this_round = True

                    # ìµœì¢… ëœë”© URL ì¶”ì 
                    final_url = ""
                    if not raw_href.startswith("javascript") and raw_href != "#" and "__click__" not in raw_href.lower():
                        final_url = await get_final_landing_url(context, raw_href, base_page_url)
                    else:
                        final_url = raw_href

                    clean_final = final_url.strip()

                    if clean_final.endswith("#") or "/board/lists" in clean_final or "dcad" in clean_final:
                        continue

                    clean_final = clean_final.replace("__CLICK__", "").replace("__click__", "")
                    if not clean_final or clean_final.lower() in ["null", "#", "http://null", "https://null"]:
                        clean_final = "ëœë”© URL ì—†ìŒ (í´ë¦­ ì´ë²¤íŠ¸)"
                    if "nstatic.dcinside.com" in clean_final:
                        clean_final = "ëœë”© URL ì—†ìŒ (ì´ë¯¸ì§€ ì„œë²„)"

                    has_img = bool(clean_img)
                    pos = get_korean_position(env, page_type, raw_pos, has_img, raw_href, clean_href + " " + clean_final)

                    ad_signature = f"{pos}|{clean_img}|{clean_final}"

                    if ad_signature not in seen:
                        seen.add(ad_signature)
                        text_val = "ì´ë¯¸ì§€ ë°°ë„ˆ" if has_img and not clean_txt else clean_txt
                        print(f"âœ… {prefix} [ìœ íš¨ {valid_attempts+1}/{max_valid}íšŒì°¨] {pos} (ìƒˆë¡œìš´ ì†Œì¬ ì¶”ê°€)")
                        collected.append({"date": today, "gallery": gallery, "env": env, "pos": pos, "url": clean_final, "img": clean_img, "text": text_val})
            except: continue

        if found_dc_ad_in_this_round:
            valid_attempts += 1
            if valid_attempts % 10 == 0:
                print(f"ğŸ“Š {prefix} [ì§„í–‰ë¥ ] ìœ íš¨ {valid_attempts}/{max_valid}íšŒ ì™„ë£Œ (ëˆ„ì  ì‹œë„: {total_attempts}, ìˆ˜ì§‘ ì†Œì¬: {len(collected)}ê°œ)")
        else:
            print(f"âš ï¸ {prefix} [ì „ì²´ êµ¬ê¸€ê´‘ê³  ë®ì„] ì¹´ìš´íŠ¸ ë¯¸ì°¨ê° (í˜„ì¬ ìœ íš¨: {valid_attempts}/{max_valid}, ëˆ„ì  ì‹œë„: {total_attempts})")

    print(f"ğŸ {prefix} ìˆ˜ì§‘ ì¢…ë£Œ! ìœ íš¨ {valid_attempts}/{max_valid}íšŒ, ì´ ì‹œë„ {total_attempts}íšŒ, ìˆ˜ì§‘ ì†Œì¬ {len(collected)}ê°œ")
    return collected

# --- âš¡ ë‹¨ì¼ ê°¤ëŸ¬ë¦¬+í™˜ê²½ ì‘ì—… ì‹¤í–‰ê¸° ---
async def task_runner(sem, ctx, env, tgt, queue):
    async with sem:
        await asyncio.sleep(random.uniform(0, 1.5))
        page = await ctx.new_page()
        page.on("dialog", lambda dialog: asyncio.create_task(dialog.accept()))
        await page.route("**/*", block_resources)
        try:
            target_url = tgt['pc'] if env == "PC" else tgt['mo']
            gallery_id = ""
            if "id=" in target_url: gallery_id = target_url.split("id=")[-1].split("&")[0]
            else: gallery_id = target_url.split("/")[-1]

            await page.goto(target_url, wait_until="load", timeout=15000)
            await asyncio.sleep(1.5)

            # ê°¤ëŸ¬ë¦¬ ì ‘ì† í™•ì¸ (ë°”ìš´ìŠ¤ ë°©ì–´)
            page_title = await page.title()
            current_url = page.url.lower()
            keyword = tgt['name'].replace("ê°¤ëŸ¬ë¦¬", "").replace(" ", "").strip()

            bounce_urls = ["https://www.dcinside.com", "https://gall.dcinside.com", "https://m.dcinside.com", "https://gall.dcinside.com/m", "https://gall.dcinside.com/mini"]
            if keyword not in page_title.replace(" ", "") or current_url in bounce_urls:
                if env == "PC":
                    test_urls = [f"https://gall.dcinside.com/board/lists/?id={gallery_id}", f"https://gall.dcinside.com/mgallery/board/lists/?id={gallery_id}", f"https://gall.dcinside.com/mini/board/lists/?id={gallery_id}"]
                    for t_url in test_urls:
                        await page.goto(t_url, wait_until="load", timeout=12000)
                        await asyncio.sleep(1)
                        if keyword in (await page.title()).replace(" ", ""): break
                elif env == "MO":
                    test_urls = [f"https://m.dcinside.com/board/{gallery_id}", f"https://m.dcinside.com/mini/{gallery_id}"]
                    for t_url in test_urls:
                        await page.goto(t_url, wait_until="load", timeout=12000)
                        await asyncio.sleep(1)
                        if keyword in (await page.title()).replace(" ", ""): break

            print(f"ğŸŒ [{env}] {tgt['name']} ì ‘ì† ì™„ë£Œ. ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹œì‘!")

            # ğŸ“‹ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ê´‘ê³  ìˆ˜ì§‘
            list_results = await capture_ads(ctx, page, env, tgt['name'], "ë¦¬ìŠ¤íŠ¸")
            for item in list_results: await queue.put(item)

            # ğŸ“ ë³¸ë¬¸ í˜ì´ì§€ ê´‘ê³  ìˆ˜ì§‘ (href ì§ì ‘ ì¶”ì¶œ â†’ ë‹¤ì´ë ‰íŠ¸ ì§„ì…)
            if env == "PC":
                post_locator = page.locator("tr.us-post:not(.notice) td.gall_tit > a:not(.reply_numbox)").first
            else:
                # ğŸ”¥ MO ì…€ë ‰í„° ìˆ˜ì •: .gall-detail-lnktit í´ë˜ìŠ¤ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŒ!
                # ì‹¤ì œ MO ê²Œì‹œê¸€ ë§í¬ëŠ” a.lt í´ë˜ìŠ¤ (ul.gall-detail-lst ë‚´ë¶€)
                post_locator = page.locator("ul.gall-detail-lst a.lt").first

            if await post_locator.count() > 0:
                post_href = await post_locator.get_attribute("href")
                if post_href:
                    if not post_href.startswith("http"):
                        base_domain = "https://gall.dcinside.com" if env == "PC" else "https://m.dcinside.com"
                        post_href = base_domain + post_href

                    # ğŸ”¥ MO í•µì‹¬ ìˆ˜ì •: PC URLì´ ë“¤ì–´ì™”ìœ¼ë©´ ëª¨ë°”ì¼ URLë¡œ ê°•ì œ ë³€í™˜
                    # PC: gall.dcinside.com/board/view/?id=toeic&no=12345
                    # MO: m.dcinside.com/board/toeic/12345
                    if env == "MO" and "gall.dcinside.com" in post_href:
                        try:
                            from urllib.parse import urlparse, parse_qs
                            parsed = urlparse(post_href)
                            qs = parse_qs(parsed.query)
                            g_id = qs.get("id", [gallery_id])[0]
                            g_no = qs.get("no", [""])[0]
                            if g_no:
                                post_href = f"https://m.dcinside.com/board/{g_id}/{g_no}"
                                print(f"ğŸ”„ [MO] PC URL â†’ ëª¨ë°”ì¼ URL ë³€í™˜: {post_href}")
                            else:
                                post_href = f"https://m.dcinside.com/board/{g_id}"
                        except:
                            post_href = post_href.replace("gall.dcinside.com", "m.dcinside.com")

                    await page.goto(post_href, wait_until="load", timeout=15000)
                    await asyncio.sleep(2.5)
                    body_results = await capture_ads(ctx, page, env, tgt['name'], "ë³¸ë¬¸")
                    for item in body_results: await queue.put(item)
        except Exception as e:
            print(f"âš ï¸ [{env}] {tgt['name']} ì „ì²´ ì—ëŸ¬: {e}")
        finally:
            try: await page.close()
            except: pass

# --- ğŸš€ ë©”ì¸ ì‹¤í–‰ ---
async def main():
    if not TARGET_GALLERIES: return
    print(f"ğŸš€ [ì„œë²„ {CHUNK_INDEX+1}/{TOTAL_CHUNKS}] ê°¤ëŸ¬ë¦¬ {len(TARGET_GALLERIES)}ê°œ ë‹´ë‹¹ ì‹œì‘!")

    gc = gspread.service_account(filename=SERVICE_ACCOUNT_FILE)
    ws = gc.open_by_url(SHEET_URL).get_worksheet(0)

    async with async_playwright() as p:
        # PC ì»¨í…ìŠ¤íŠ¸: ì¼ë°˜ Windows Chrome ìœ„ì¥
        pc_context_opts = {
            "viewport": {"width": 1920, "height": 1080},
            "user_agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
        }
        # ğŸ”¥ MO ì»¨í…ìŠ¤íŠ¸: ê°¤ëŸ­ì‹œ Android Mobile Chrome ì™„ë²½ ìœ„ì¥
        mo_context_opts = {
            "user_agent": "Mozilla/5.0 (Linux; Android 13; SM-G991N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Mobile Safari/537.36",
            "viewport": {"width": 390, "height": 844},
            "is_mobile": True,
            "has_touch": True
        }

        browser = await p.chromium.launch(headless=True, args=["--disable-blink-features=AutomationControlled", "--no-sandbox", "--disable-web-security"])
        pc_ctx = await browser.new_context(**pc_context_opts)
        mo_ctx = await browser.new_context(**mo_context_opts)

        sem, queue = asyncio.Semaphore(5), asyncio.Queue()
        uploader = asyncio.create_task(uploader_worker(queue, ws))

        tasks = [task_runner(sem, pc_ctx, "PC", t, queue) for t in TARGET_GALLERIES] + \
                [task_runner(sem, mo_ctx, "MO", t, queue) for t in TARGET_GALLERIES]
        await asyncio.gather(*tasks)
        await browser.close()

        await queue.put(None)
        await uploader
        print(f"ğŸ‰ [ì„œë²„ {CHUNK_INDEX+1}] ìˆ˜ì§‘ ì¢…ë£Œ!")

if __name__ == "__main__": asyncio.run(main())
