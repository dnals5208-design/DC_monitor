import asyncio
import random
import time
from playwright.async_api import async_playwright
import gspread
from datetime import datetime

# --- âš™ï¸ ì„¤ì • ---
SERVICE_ACCOUNT_FILE = 'service_account2020.json' 
SHEET_URL = 'https://docs.google.com/spreadsheets/d/1omDVgsy4qwCKZMbuDLoKvJjNsOU1uqkfBqZIM7euezk/edit?gid=0#gid=0'

# ğŸ“ í™•ì¥ëœ 37ê°œ ê°¤ëŸ¬ë¦¬ ë¦¬ìŠ¤íŠ¸
TARGET_GALLERIES = [
[
    {"name": "4ë…„ì œëŒ€í•™ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=4year_university", "mo": "https://m.dcinside.com/board/4year_university"},
    {"name": "7ê¸‰ê³µì±„ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=neo7gall", "mo": "https://m.dcinside.com/board/neo7gall"},
    {"name": "HSKê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=hsk123456", "mo": "https://m.dcinside.com/board/hsk123456"},
    {"name": "JLPTê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=jlpt", "mo": "https://m.dcinside.com/board/jlpt"},
    {"name": "ê³ ì‹œì‹œí—˜ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=exam_new", "mo": "https://m.dcinside.com/board/exam_new"},
    {"name": "ê³µë¬´ì›ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=government", "mo": "https://m.dcinside.com/board/government"},
    {"name": "ê³µì¸ì¤‘ê°œì‚¬ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=lreaexam", "mo": "https://m.dcinside.com/board/lreaexam"},
    {"name": "êµ°ë¬´ì›ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=soider", "mo": "https://m.dcinside.com/board/soider"},
    {"name": "ëŒ€í•™ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=univ_new", "mo": "https://m.dcinside.com/board/univ_new"},
    {"name": "ë“€ì˜¤ë§ê³ ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=duolingo", "mo": "https://m.dcinside.com/board/duolingo"},
    {"name": "ëŸ¬ì‹œì•„ì–´ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=russiangall", "mo": "https://m.dcinside.com/board/russiangall"},
    {"name": "ë§ˆì´ìŠ¤í„°ê³ ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=meister", "mo": "https://m.dcinside.com/board/meister"},
    {"name": "ë²•í•™ì „ë¬¸ëŒ€í•™ì›ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=lawschool", "mo": "https://m.dcinside.com/board/lawschool"},
    {"name": "ì„¸ë¬´ì‚¬ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=cta", "mo": "https://m.dcinside.com/board/cta"},
    {"name": "ì†Œë°©ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=fire", "mo": "https://m.dcinside.com/board/fire"},
    {"name": "ìˆœê²½ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=policeofficer", "mo": "https://m.dcinside.com/board/policeofficer"},
    {"name": "ì–´í•™ì—°ìˆ˜ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=language", "mo": "https://m.dcinside.com/board/language"},
    {"name": "ì˜ì–´ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=English", "mo": "https://m.dcinside.com/board/English"},
    {"name": "ì˜ì–´íšŒí™”ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=englishspeech", "mo": "https://m.dcinside.com/board/englishspeech"},
    {"name": "ì˜¤í”½ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=opic", "mo": "https://m.dcinside.com/board/opic"},
    {"name": "ìœ í•™ì‹œí—˜ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=eju", "mo": "https://m.dcinside.com/board/eju"},
    {"name": "ì¼ì–´ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=japanese", "mo": "https://m.dcinside.com/board/japanese"},
    {"name": "ì„ìš©ê³ ì‹œê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=imyoung", "mo": "https://m.dcinside.com/board/imyoung"},
    {"name": "ìê²©ì¦ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=coq", "mo": "https://m.dcinside.com/board/coq"},
    {"name": "ì „ì‚°ì„¸ë¬´íšŒê³„ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=accounting", "mo": "https://m.dcinside.com/board/accounting"},
    {"name": "ì •ë³‘ê¶Œê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=jeongbk", "mo": "https://m.dcinside.com/board/jeongbk"},
    {"name": "ì¤‘êµ­ì–´ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=chinese", "mo": "https://m.dcinside.com/board/chinese"},
    {"name": "ì§€í…”í”„ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=gtelf", "mo": "https://m.dcinside.com/board/gtelf"},
    {"name": "ì»´í“¨í„°í™œìš©ëŠ¥ë ¥ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=itlicense", "mo": "https://m.dcinside.com/board/itlicense"},
    {"name": "í…ìŠ¤ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=teps", "mo": "https://m.dcinside.com/board/teps"},
    {"name": "í† ìµê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=toeic", "mo": "https://m.dcinside.com/board/toeic"},
    {"name": "í† ìµìŠ¤í”¼í‚¹ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=toespic", "mo": "https://m.dcinside.com/board/toespic"},
    {"name": "í† í”Œê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=toefl", "mo": "https://m.dcinside.com/board/toefl"},
    {"name": "í¸ì…ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=admission", "mo": "https://m.dcinside.com/board/admission"},
    {"name": "í•™ì ì€í–‰ì œê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=acbs", "mo": "https://m.dcinside.com/board/acbs"},
    {"name": "í•´ì–‘ê²½ì°°ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=kcg", "mo": "https://m.dcinside.com/board/kcg"},
    {"name": "íšŒê³„ì‚¬ê°¤ëŸ¬ë¦¬", "pc": "https://gall.dcinside.com/board/lists/?id=cpa", "mo": "https://m.dcinside.com/board/cpa"}
]

# ğŸ¯ êµ¬ê¸€ ì‹œíŠ¸ ì•ˆì „ ì—…ë¡œë“œ í•¨ìˆ˜ (ë™ê¸° ì²˜ë¦¬ìš©)
def safe_batch_upload(ws, data_chunk):
    if not data_chunk: return
    rows_to_append = [[d['date'], d['gallery'], d['env'], d['pos'], d['url'], d['img'], d['text']] for d in data_chunk]
    
    # ë°ì´í„°ë¥¼ 30ê°œì”© ìª¼ê°œì„œ ì—…ë¡œë“œ (API ë³´í˜¸)
    for i in range(0, len(rows_to_append), 30):
        sub_chunk = rows_to_append[i : i + 30]
        try:
            ws.append_rows(sub_chunk)
            time.sleep(1.5) # ê¿€ë§› íœ´ì‹ (API ì—ëŸ¬ ì›ì²œ ì°¨ë‹¨)
        except Exception as e:
            print(f"\nâš ï¸ êµ¬ê¸€ ì‹œíŠ¸ ì—…ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            time.sleep(5) # ì—ëŸ¬ ì‹œ 5ì´ˆ ëŒ€ê¸° í›„ ë‹¤ìŒ ì‘ì—… ì§„í–‰

# ğŸ¯ 250ê°œ ë„ë‹¬ ì‹œ ì—…ë¡œë“œë¥¼ ë‹´ë‹¹í•˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ (Consumer)
async def uploader_worker(queue, ws):
    buffer = []
    total_uploaded = 0
    print("\nğŸ“¦ [ì‹œìŠ¤í…œ] ë°±ê·¸ë¼ìš´ë“œ ì—…ë¡œë“œ ë§¤ë‹ˆì € ê°€ë™ ì™„ë£Œ (250ê°œ ë‹¨ìœ„ ëŒ€ê¸° ì¤‘...)")
    
    while True:
        item = await queue.get()
        if item is None: # Noneì´ ë“¤ì–´ì˜¤ë©´ ëª¨ë“  íƒìƒ‰ì´ ëë‚¬ë‹¤ëŠ” ì‹ í˜¸
            break
            
        buffer.append(item)
        
        # ë°”êµ¬ë‹ˆì— 250ê°œê°€ ì°¨ë©´ ì¦‰ì‹œ ì—…ë¡œë“œ ì‹¤ì‹œ!
        if len(buffer) >= 250:
            print(f"\nğŸš€ [ì‹œìŠ¤í…œ] ë°ì´í„° 250ê°œ ë„ë‹¬! êµ¬ê¸€ ì‹œíŠ¸ ì¤‘ê°„ ì—…ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (30ê°œì”© ë¶„í• )...")
            await asyncio.to_thread(safe_batch_upload, ws, buffer)
            total_uploaded += len(buffer)
            print(f"âœ… [ì‹œìŠ¤í…œ] ì¤‘ê°„ ì—…ë¡œë“œ ì™„ë£Œ. (ëˆ„ì  ì—…ë¡œë“œ: {total_uploaded}ê±´)")
            buffer.clear()
            
        queue.task_done()
        
    # íƒìƒ‰ ì¢…ë£Œ í›„ ë°”êµ¬ë‹ˆì— ë‚¨ì€ ì”ì—¬ ë°ì´í„°(250ê°œ ë¯¸ë§Œ) ìµœì¢… ì—…ë¡œë“œ
    if buffer:
        print(f"\nğŸš€ [ì‹œìŠ¤í…œ] íƒìƒ‰ ì™„ì „ ì¢…ë£Œ. ë‚¨ì€ ìíˆ¬ë¦¬ ë°ì´í„° {len(buffer)}ê±´ì„ ìµœì¢… ì—…ë¡œë“œí•©ë‹ˆë‹¤...")
        await asyncio.to_thread(safe_batch_upload, ws, buffer)
        total_uploaded += len(buffer)
        
    print(f"\nğŸ‰ [ì‹œìŠ¤í…œ] ëª¨ë“  êµ¬ê¸€ ì‹œíŠ¸ ì—…ë¡œë“œ ì‘ì—…ì´ ì•ˆì „í•˜ê²Œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (ì´ {total_uploaded}ê±´)")

def get_korean_position(env, page_type, raw_pos, img_src):
    raw = str(raw_pos).lower()
    if not img_src: return "í…ìŠ¤íŠ¸ë°°ë„ˆ"
    if "icon" in raw or "float" in raw or "pop-layer" in raw: return "ì•„ì´ì½˜ë°°ë„ˆ"
    
    if env == "PC":
        if page_type == "ë³¸ë¬¸": 
            if "bottom" in raw or "btm" in raw: return "í•˜ë‹¨ë°°ë„ˆ"
            return "ê²Œì‹œê¸€ë°°ë„ˆ"
        else: 
            if "right" in raw or "wing" in raw: return "ìš°ì¸¡ë°°ë„ˆ"
            if "left" in raw: return "ì¢Œì¸¡ë°°ë„ˆ"
            if "bottom" in raw or "btm" in raw: return "í•˜ë‹¨ë°°ë„ˆ"
            return "ìƒë‹¨ë°°ë„ˆ"
    else: 
        if page_type == "ë³¸ë¬¸":
            if "bottom" in raw or "btm" in raw: return "í•˜ë‹¨ë°°ë„ˆ"
            return "ê²Œì‹œê¸€ë°°ë„ˆ"
        else:
            if "bottom" in raw or "btm" in raw: return "í•˜ë‹¨ë°°ë„ˆ"
            return "ìƒë‹¨ë°°ë„ˆ"

async def get_final_landing_url(context, redirect_url):
    if not redirect_url or not redirect_url.startswith("http"): return redirect_url
    if "addc.dcinside" not in redirect_url and "NetInsight" not in redirect_url: return redirect_url
    try:
        temp_page = await context.new_page()
        await temp_page.goto(redirect_url, wait_until="commit", timeout=4000)
        final_url = temp_page.url
        await temp_page.close()
        return final_url
    except:
        return redirect_url

async def block_unnecessary_resources(route):
    req_url = route.request.url
    if route.request.resource_type in ["font", "media", "stylesheet"]:
        await route.abort()
    elif route.request.resource_type == "image" and not any(k in req_url for k in ["dcinside", "toast.com", "ads"]):
        await route.abort()
    else:
        await route.continue_()

async def capture_all_visible_ads(context, page, env, gallery_name, page_type):
    collected = []
    seen_keys = set()
    today_str = datetime.now().strftime("%Y-%m-%d")
    prefix = f"[{env}|{gallery_name[:4]}|{page_type}]"
    
    valid_refreshes = 0
    attempt = 0
    
    while valid_refreshes < 10 and attempt < 30:
        attempt += 1
        found_ad_in_this_round = False
        current_round = valid_refreshes + 1
        ad_count_in_round = 0 
        
        try:
            await page.reload(wait_until="domcontentloaded", timeout=12000)
            await asyncio.sleep(2) 
            if page_type == "ë³¸ë¬¸":
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
                await asyncio.sleep(1)
        except: pass

        for frame in page.frames:
            try:
                ads = await frame.locator("a").all()
                for ad in ads:
                    href = await ad.get_attribute("href") or ""
                    img = ad.locator("img")
                    img_src = await img.first.evaluate("node => node.src") if await img.count() > 0 else ""
                    raw_pos = await ad.evaluate("node => { let p = node.closest('div'); return p ? p.className : 'unknown'; }")
                    text_content = await ad.inner_text() or ""
                    
                    if "google" in href.lower() or "adsrvr.org" in href.lower() or "criteo" in href.lower(): continue
                    if "googleactiveview" in str(raw_pos).lower(): continue
                    if "dc/w/images" in img_src or "info_polic" in href or "close" in img_src.lower(): continue
                    if href == "#" or "javascript" in href.lower() or not href: continue
                        
                    if "nstatic.dcinside.com/dc/" in href or "policy" in href or "useinfo" in href or "dcad" in href: continue
                    if any(word in text_content for word in ["ì´ìš©ì•ˆë‚´", "ì´ìš©ì•½ê´€", "ê°œì¸ì •ë³´", "ì²­ì†Œë…„ë³´í˜¸", "ê´‘ê³ ì•ˆë‚´"]): continue

                    is_ad = any(k in href or k in (img_src or "") for k in ["addc.dcinside", "NetInsight", "nstatic.dcinside", "toast.com"])
                    
                    if is_ad:
                        found_ad_in_this_round = True
                        key = img_src if img_src else href
                        
                        if key not in seen_keys:
                            seen_keys.add(key)
                            ad_count_in_round += 1
                            final_url = await get_final_landing_url(context, href)
                            text_val = (await img.first.get_attribute("alt") if img_src else text_content) or "ì´ë¯¸ì§€ ë°°ë„ˆ"
                            korean_pos = get_korean_position(env, page_type, raw_pos, img_src)
                            
                            print(f"âœ… {prefix} [{current_round}íšŒì°¨ ìƒˆë¡œê³ ì¹¨ - {ad_count_in_round}ë²ˆì§¸ ë°œê²¬] {korean_pos}")
                            
                            collected.append({
                                "date": today_str, "gallery": gallery_name, "env": env,
                                "pos": korean_pos, "url": final_url, "img": img_src, "text": text_val.strip()
                            })
            except: continue
        
        if found_ad_in_this_round:
            valid_refreshes += 1
            
    return collected

# âš¡ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ìë§ˆì í(Queue) ë°”êµ¬ë‹ˆì— ì§‘ì–´ë„£ëŠ” ì—­í• 
async def run_scraper_task(sem, context, env, target, data_queue):
    async with sem:
        await asyncio.sleep(random.uniform(0, 2.0)) 
        page = await context.new_page()
        await page.route("**/*", block_unnecessary_resources)
        
        try:
            url = target['pc'] if env == "PC" else target['mo']
            await page.goto(url, wait_until="domcontentloaded", timeout=15000)
            
            # ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ë°ì´í„° ìˆ˜ì§‘ ë° íì— ì „ì†¡
            list_data = await capture_all_visible_ads(context, page, env, target['name'], "ë¦¬ìŠ¤íŠ¸")
            for item in list_data:
                await data_queue.put(item)
            
            # ë³¸ë¬¸ í˜ì´ì§€ ì´ë™
            post = page.locator("tr.us-post:not(.notice) td.gall_tit > a:not(.reply_numbox)").first if env == "PC" else page.locator("ul.gall-detail-lst li:not(.notice) .gall-detail-lnktit a").first
            if await post.count() > 0:
                await post.click()
                await asyncio.sleep(1.5)
                
                # ë³¸ë¬¸ í˜ì´ì§€ ë°ì´í„° ìˆ˜ì§‘ ë° íì— ì „ì†¡
                body_data = await capture_all_visible_ads(context, page, env, target['name'], "ë³¸ë¬¸")
                for item in body_data:
                    await data_queue.put(item)
        except: pass
        finally: await page.close()

async def main():
    print("==================================================")
    print("ğŸš€ [ëŒ€ê·œëª¨ 37ê°œ ê°¤ëŸ¬ë¦¬] ì´ˆê³ ì† ë³‘ë ¬ ìˆ˜ì§‘ & ì‹¤ì‹œê°„ ë¶„í•  ì—…ë¡œë“œ ê°€ë™")
    print("==================================================")
    
    # ğŸ§¹ 1ë‹¨ê³„: ì‹œíŠ¸ ì‚¬ì „ ì²­ì†Œ (ê³¼ê±° ë°ì´í„° ë³´ì¡´, ì˜¤ëŠ˜ ë°ì´í„°ë§Œ ì´ˆê¸°í™”)
    print("\nğŸ§¹ êµ¬ê¸€ ì‹œíŠ¸ ì‚¬ì „ ì´ˆê¸°í™” ì¤‘ (ì˜¤ëŠ˜ ì¤‘ë³µ ë°ì´í„° ë°©ì§€)...")
    gc = gspread.service_account(filename=SERVICE_ACCOUNT_FILE)
    ws = gc.open_by_url(SHEET_URL).get_worksheet(0)
    
    existing_rows = ws.get_all_values()
    today_str = datetime.now().strftime("%Y-%m-%d")
    headers = ["ë‚ ì§œ", "ê°¤ëŸ¬ë¦¬ëª…", "í™˜ê²½", "ìœ„ì¹˜", "URL", "ì´ë¯¸ì§€", "í…ìŠ¤íŠ¸ë¬¸êµ¬"]
    kept_rows = [headers]
    
    if existing_rows:
        for row in existing_rows[1:]:
            if len(row) > 0 and row[0] != today_str: 
                kept_rows.append(row)
                
    ws.clear()
    
    # ë‚¨ê²¨ì§„ ê³¼ê±° ë°ì´í„°ê°€ ìˆë‹¤ë©´ 100ê°œì”© ìª¼ê°œì„œ ë³µêµ¬ (API ë³´í˜¸)
    if len(kept_rows) > 1:
        print("   â–¶ï¸ ê³¼ê±° íˆìŠ¤í† ë¦¬ ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ë³µêµ¬í•©ë‹ˆë‹¤.")
        for i in range(0, len(kept_rows), 100):
            ws.append_rows(kept_rows[i:i+100])
            time.sleep(1.5)

    # ğŸš€ 2ë‹¨ê³„: ìˆ˜ì§‘ ë° ì‹¤ì‹œê°„ ì—…ë¡œë“œ ì‹œì‘
    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            args=["--disable-blink-features=AutomationControlled", "--disable-dev-shm-usage", "--no-sandbox"]
        )
        ua = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
        
        pc_context = await browser.new_context(viewport={"width": 1920, "height": 1080}, user_agent=ua)
        mo_context = await browser.new_context(**p.devices['iPhone 13']) 

        sem = asyncio.Semaphore(8)
        data_queue = asyncio.Queue() # ì‹¤ì‹œê°„ ë°ì´í„° ë°”êµ¬ë‹ˆ ìƒì„±

        # ë°±ê·¸ë¼ìš´ë“œ ì—…ë¡œë” ì‹¤í–‰ (ë´‡ë“¤ì´ ì¼í•˜ëŠ” ë™ì•ˆ ë’¤ì—ì„œ ëŒ€ê¸°)
        uploader_task = asyncio.create_task(uploader_worker(data_queue, ws))

        # 37ê°œ ê°¤ëŸ¬ë¦¬ (ì´ 74ê°œ ì‘ì—…) ë³‘ë ¬ ì¶œë°œ
        tasks = []
        for target in TARGET_GALLERIES:
            tasks.append(run_scraper_task(sem, pc_context, "PC", target, data_queue))
            tasks.append(run_scraper_task(sem, mo_context, "MO", target, data_queue))

        # ëª¨ë“  ë´‡ë“¤ì˜ íƒìƒ‰ì´ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
        await asyncio.gather(*tasks, return_exceptions=True)
        await browser.close()
        
        # ë´‡ íƒìƒ‰ ì¢…ë£Œ â¡ï¸ ì—…ë¡œë”ì—ê²Œ ì¢…ë£Œ ì‹ í˜¸(None) ì „ì†¡
        await data_queue.put(None)
        
        # ì—…ë¡œë”ê°€ ë‚¨ì€ ìíˆ¬ë¦¬ ë°ì´í„°ë¥¼ ëª¨ë‘ ì‹œíŠ¸ì— ì˜¬ë¦´ ë•Œê¹Œì§€ ëŒ€ê¸°
        await uploader_task

if __name__ == "__main__":
    asyncio.run(main())
